<details>
  <summary>Сентябрь 2025 (9 штук)</summary>

<details>
  <summary>Яндекс</summary>

пт
# [1] Задача 
 ```sql
with tmp as (select user_id, order_id,
row_number() over (partition by user_id order by order_timestamp) as rn
from taxi)
select user_id, order_id from tmp
where rn = 1

 ```
 # [2] Задача 
 ```sql
select t1.car_number
from sandbox.zvereva_taxi_orders t1 join sandbox.zvereva_taxi_orders t2 on t1.car_number = t2.car_number
and t1.endd>t2.start and t1.start<t2.endd

 ```
 # [3] Задача 
 ```python
'''1 вариант:'''

from collections import Counter

def find_mode (arr: list):
  arr = Counter(arr)
  return max (counter, key = counter.get)

'''2 вариант'''

def find_mode (arr: list):
  moda_count = 0
  k = 0
  arr = sorted(arr)
  for i in range (len(arr)-1):
    if arr[i] == arr[i+1]:
      k += 1
    else:
      if k > moda_count:
        moda_count = k
        moda = arr[i]
      k = 0
  return moda


 ```
 # [4] Задача 
 ```python

def func(d:dict):
  d_new = {}
  arr_keys = list(d.keys())
  arr_values = list(d.values())
  for i in range(len(d)):
    d_new.update ({ arr_values[i] : arr_keys[i]})

  return d_new

 ```

</details>

<details>
  <summary>ЭР-Телеком</summary>

сб
 # [1] Задача 
[1, 2, 3, 1, 2, 3]

 # [2] Задача 
arr = [x*2 for x in arr]

 # [3] Задача 

 1 
 2
 3 


 1

# [4] Задача 

Каждому человеку даем 2 стаканчика, он их пробует всплепую, очередность стаканчиков определяется рандомом. Затем собираем выборку из большого числа наблюдений. Считаем кол-во голосов за каждый вид колы, а затем с помощью стат теста проверяем, значимы ли статистически результаты. Стат тест берем биномиальный, потому что он подходит для бинарных выводов

(задача на двойное слепое тестирование)


</details>



<details>
  <summary>Т-банк</summary>

вс
# [1] Задача 

 ```sql
select t1.id, t1.department_id
from employee t1
join employee t2
on t1.department_id = t2.department_id 
and t1.chief_flg = False
and t2.chief_flg = True
and t1.birth_dt < t2.birth_dt
group by t1.id, t1.department_id
 ```

 # [2] Задача 

 ```sql
with tmp as (
  select transaction_id, customer_id, amount_rur, transaction_dttm,
  row_number() over (partition by customer_id order by transaction_dttm) as rn,
  sum(amount_rur) over (partition by customer_id) as sm
  from transactions
  where success_flg = True
)
select  transaction_id, customer_id, amount_rur, transaction_dttm
from tmp
where rn = 1 and sm > 1000000
 ```

  # [3] Задача 

 ```sql
with tmp as (
  select transaction_id, customer_id, amount_rur,
  lag(amount_rur) over (partition by customer_id order by transaction_dttm) as lg,
  lead(amount_rur) over (partition by customer_id order by transaction_dttm) as ld
)
select transaction_id, customer_id, amount_rur
from tmp
where amount_rur > lg and amount_rur < ld
 ```

# [4] Задача 

 ```sql
with tmp as (select client_id, sum(amount) as sm
from transaction t join account a on t.account_id = a.id
where transaction_date > current_date - interval '1 month'
and type = 'покупка'
group by client_id)
select distinct name
from client c join account a2 on c.id = a2.client_id
where close_dt is null
and c.id in (select client_id from tmp where sm < 5000)
and open_dt < current_date - interval '1 year'
 ```

# [5] Задача 

 ```sql
SELECT 
    c1.id,
    c1.points,
    COUNT(DISTINCT c2.points) + 1 as position
FROM sandbox.zvereva_competition c1
LEFT JOIN sandbox.zvereva_competition c2 ON c2.points > c1.points
GROUP BY c1.id, c1.points
ORDER BY c1.points DESC;
 ```
</details>


<details>
  <summary>Лига цифровой экономики</summary>

пн

# [1] Задача 

t1: 3   t2: 4
INNER JOIN
min: 0
max: 12

LEFT JOIN
min: 3
max: 12

RIGHT JOIN
min: 4
max: 12

FULL JOIN
min: 4
max: 12

CROSS JOIN
min: 12
max: 12


 # [2] Задача 

 ```sql
SELECT value, count(value) as cnt
from T
group by value
having count(value) > 1
 ```

# [3] Задача 

 ```sql
tmp as (SELECT empl_fio, empl_dep, amount,
row_number() over (partition by empl_dep order by amount desc) as rn
from salary
where extract(month from value_day) = 4
and extract(year from value_day) = 2023)
select empl_fio, empl_dep, amount from tmp
where rn = 2
 ```

 # [4] Задача 

 ```sql
with tmp as (select empl_fio, empl_dep, amount as may_amount,
dense_rank() over (order by amount) as rank
from salary
where extract(month from value_day) = 5
and extract(year from value_day) = 2023),

apr as (
  select empl_fio, empl_dep, amount as april_amount, value_day
  from salary
  where extract(month from value_day) = 4
and extract(year from value_day) = 2023) 


select t.empl_fio, t.empl_dep, (may_amount - april_amount) as diff, may_amount
from tmp t join apr a on t.empl_fio = a.empl_fio and t.empl_dep = a.empl_dep 
where rank <= 3
 ```
</details>

<details>
  <summary>Wildberries</summary>

вт

 # [1] Задача 

 ```sql
select t1.id, t1.name, t1.salary, t2.salary
from employee t1 join employee t2 on t1.manager_id = t2.id and t1.salary > t2.salary

 ```

 # [2] Задача 

 ```sql
with tmp as (select id, name, salary, departmentID,
dense_rank() over (partition by departmentID order by salary desc) as rk
from employee)
select name
from tmp
where rk <= 3
 ```
# [3] Задача 

 ```python
'''Напишите функцию, которая принимает на вход список hours (содержащий N элементов) и целое число target.
Функция должна возвращать количество сотрудников, отработавших не меньше target часов'''

def func(hours, target):
  cnt = 0
  for i in range(len(hours)):
    if hours[i] >= target:
      cnt += 1
  return cnt
 ```

 # [3] Задача 

 ```python
df['DepositionSum'].mean()
df['WithdrawalSum'].mean()

df.groupby('Day').agg({'DepositionSum': 'mean',
                       'WithdrawalSum': 'mean'}).mean()

df.groupby('UserId').agg({'DepositionSum': 'mean',
                       'WithdrawalSum': 'mean'}).mean()

df['DepositionSum'].sum() - df['WithdrawalSum'].sum()

'''Сегментация: k-means, по сумме депозита, по частоте'''

 ```

</details>

<details>
  <summary>Бетсити</summary>

ср

 # [1] Задача 

 d

 # [2] Задача 

 bc

b. TRUNCATE TABLE my_table - удаляет все строки, быстрее, нельзя откатить
c. DELETE FROM my_table - удаляет все строки, можно использовать с WHERE, можно откатить
a. DROP TABLE my_table - удаляет саму таблицу
d. DELETE my_table - неверный синтаксис (требуется FROM)

 # [3] Задача 

(4500*20+1500*50+2500*30)/100
2400

 # [4] Задача 

Доходы с контрольной группы: 1500
Доходы с целевой группы: 18000
Расходы на рассылку контроль: 500
Расходы на рассылку целевая: 4500

Прибыль со старой конверсией:
10000*0.03*50 = 15000

Прибыль с новой конверсией:
10000*0.04*50 - 5000 = 15000

Итого: эффекта нет


# [5] Задача 

 ```sql
select t1.name, t1.salary, t2.name, t2.salary, t3.name
from employee t1 join employee t2 on t1.chief_id = t2.id 
join department t3 on t1.department_id = t3.id
where t2.salary / t1.salary between 1.5 and 2
 ```


# [6] Задача 

 ```sql
select extract (month from sale_dttm) as month, extract (year from sale_dttm) as year,
percentile_cont(0.5) within group (order by price * (1-discount/100))
from tmp
group by year, month
order by year, month
 ```



</details>


<details>
  <summary>Wildberries</summary>

чт

 # [1] Задача 

 ```sql
with tmp as (select courier_id, pickup_point,
avg(delivery_time) as ag
from orders
where delivery_time <= 120
group by courier_id, pickup_point),

tmp_2 as (select pickup_point, courier_id, 
row_number() over (partition by pickup_point order by ag) as rn
from tmp)

select pickup_point, courier_id from tmp_2
where rn = 1
 ```

# [2] Задача 

 ```sql
with tmp as (select order_date, courier_id, avg(delivery_time) as ag_days
from deliveries
group by order_date, courier_id)

select order_date, courier_id,
avg (ag_days) over (partition by courier_id order by order_date rows between 6 preceding and current row)
from tmp
 ```

# [3] Задача 

 ```python
 import pandas as pd
 df = pd.read_csv('Таблица')

 df.groupby('courier_id').agg({'delivery_time':'mean', 'distance':'mean'})

df['efficiency'] = df['distance']/df['delivery_time']
df.groupby(['start_location', 'end_location']).agg({'efficiency': 'mean'})

df.groupby('courier_id')['efficiency'].mean().sort_values(ascending=False)
 ```

</details>

<details>
  <summary>HeadHunter</summary>

пт

# [1] Задача 

 ```sql
with tmp as (
  select product_id, extract(year from sale_date) as yr, extract(month from sale_date) as mh,
sum(amount) over (partition by product_id, extract(year from sale_date), extract(month from sale_date)) as sm
from sales),
tmp_2 as (
  select product_id, yr, mh, sm,
  lag(sm) over (partition by product_id order by yr, mh) as lg
  from tmp
)
select product_id, yr, mh, coalesce((sm-lg)/nullif(lg,0),0)*100 as final
from tmp_2
order by yr, mh
 ```

# [2] Задача 

 ```python
def func(x):
  cnt = 0
  if not isinstance (x,int):
    return 'Не целое число'
  if x < 0:
    return 'Отрицательное число не может быть составным'
  elif x == 1 or x == 0:
    return 'Ни простое, ни составное'
  else:
    for i in range (2, x+1):
      if x%i == 0:
        cnt += 1
    if cnt == 1:
      return 'Простое'
    else:
      return 'Составное'
 ```

 # [3] Задача 

 ```python
def func(n):
  nums = [0, 1]
  for i in range(2,n+1):
    nums.append(nums[i-1] + nums[i - 2])
  return nums[n]
 ```
 # [4] Задача 

Чему равна вероятность выпадения двух шестерок на двух игральных костях, если сумма выпавших очков четна

С - сумма выпавших очков четна
P(C) = 1/2
так как
18*2 - всего случаев
четных - 18

Р(А) - вероятность выпадения двух шестерок
1/36 

Р(С|A) - вероятность выпасть четному кол-ву очков, если выпали 2 шестерки = 1

Р(А|C) = Р(В|A)*P(A)/P(C) = 1/36 * 2/1 = 1/18

# [5] Задача 

Кидаем 5 раз монетку. Какая вероятность того, что Орел выпадет 3 раза из 5 в любом порядке?

Кол-во благоприятных исходов: С(3,5) = 5!/2!*3! = 10
Кол-во всех исходов: 2*2*2*2*2 = 32
Ответ: 10/32 = 5/16

</details>

<details>
  <summary>CloudPayments</summary>

сб
 # [1] Задача 

 ```sql
select distinct student_id from students 
where student_id not in(
  select student_id from students s join marks m on s.id = m.student_id 
  where mark in (1,2,3)
)
 ```

 # [2] Задача 

 ```sql

with tmp as (select m.id, 
sum(amount) over (partition by m.id) as sm,
row_number() over (partition by m.id order by date_trans desc) as rn
from merchant m inner join merch_transaction t on m.id = t.merch_id
where type = 'Успешно')
select id, sm from tmp
where rn = 1

 ```
 # [3] Задача 

 ```sql
select id, 
sum(amount) over (partition by m.id order by date_trans) as sm
from merchant m inner join merch_transaction t on m.id = t.merch_id
where type = 'Успешно'
 ```
 # [4] Задача 

 ```python
df_final = df_amounts.groupby('scopeid')['amount'].sum().sort_values(ascending = False).head(3)
df_final = df_final.reset_index()
df_final = df_final.merge(df_names, on = 'scopeid', how = 'left')
df_final[['scope_name', 'amount']]
 ```

</details>


</details>


<details>
  <summary>Ноябрь 2025 (2 штуки)</summary>

<details>
  <summary>Точка 2</summary>
  вс

 # [1] Задача 

- Объяснить, в чём принципиальная разница в результате между этими двумя запросами.

В ON - фильтрация выполняется во время соединения, то есть из второй таблицы не возьмутся строки с id>=5
В where - фильтрация происходит уже после соединения, то есть из всей готовой таблицы удалятся те строки, где b.id>=5 

- На небольшом примере данных (в том числе с значениями 2, 5, 10) показать, какие строки попадут в результат каждого запроса.

2 кот    
5 собака
10 мышь

2 кот
7 крыса
10 мышь

вариант 1 с where:
10 мышь мышь

вариант 2 с on:

2 кот null
5 собака null
10 мышь мышь

# [2] Задача 

 ```sql

-- Написать SQL-скрипт, который позволит однозначно ответить на вопрос: совпадают ли результаты этих двух выборок или нет

with diff as (select a.id, b.id
from a full outer join b on a.id = b.id or (a.id is Null and b.id is Null)
where a.id is distinct from b.id)
select 
case
when exists (select True from diff) then 'Отличия есть'
else 'Отличий нет'
end as result
 ```
# [3] Задача - Продуктовый кейс: отдел обзвона клиентов

В компании есть отдел, который обзванивает клиентов и продаёт им продукт. Руководству нужно понять, оставлять ли этот отдел в следующем году или закрывать.

Известно:

Оператор делает около 100 звонков в день.
Конверсия в подключение по результатам обзвона — 5%.
С одной продажи компания получает 3000 ₽ (считать это чистой маржой).
Зарплата сотрудника — порядка 70 000 ₽ в месяц + налоги (итого около 100 000 ₽ совокупных затрат).
Без обзвона есть «естественная» конверсия клиентов в подключение — 3%.
Нужно:

Предложить, какие метрики и расчёты нужно сделать, чтобы решить, выгоден ли отдел обзвона.
Показать, как учесть естественную конверсию (3%) и выделить именно дополнительный эффект от работы оператора.
Сравнить дополнительную выручку/маржу с затратами на отдел и сделать вывод, стоит ли его оставлять.


1) Если после обзвона конверсия 5, а естественная 3, то тогда эти +2% - полученная выгода

100 * 0.02 * 3000 = 6000 - это прибыль с одного сотрудника за день

за месяц с одного сотрудника прибыль: 6000 * 22 рабочих дня - 100000 = 32 000

2) Можно предложить метрику ROMI

(доп прибыль)/ затраты = 132000/100000 = 132% (> 100%, значит вложения окупаются)
Решение: оставлять отдел обзвона
 </details>

<details>
  <summary>Билайн</summary>
  пн


# [1] Задача  Сумма положительных и отрицательных значений

 ```sql
--Есть таблица values_table с единственным числовым полем v. В этом поле могут быть как положительные, так и отрицательные значения. Требуется написать SQL-запрос, который посчитает:

-- сумму всех положительных значений поля v;
-- сумму всех отрицательных значений поля v.

SELECT 
    SUM(CASE WHEN v > 0 THEN v ELSE 0 END) AS sum_posit,
    SUM(CASE WHEN v < 0 THEN v ELSE 0 END) AS sum_negat
FROM values_table;
 ```


# [2] Задача Группировка чисел по диапазонам

 ```sql

-- Используя ту же таблицу values_table(v), нужно сгруппировать все значения по диапазонам одинаковой ширины, например по 1000 единиц: от -2000 до -1000, от -1000 до 0, от 0 до 1000, от 1000 до 2000 и т.д. Требуется написать SQL-запрос, который:

-- относит каждое значение v к соответствующему диапазону;
-- считает количество значений в каждом диапазоне;
-- выводит только те диапазоны, в которых есть хотя бы одно значение.

with tmp as (select v,
case 
when v >= -2000 and v < -1000 then 'от -2000 до -1000'
when v >= -1000 and v < 0 then 'от -1000 до 0'
when v >= 0 and v < 1000 then 'от 0 до 1000'
when v >= 1000 and v <= 2000 then 'от 1000 до 2000'
end as group_range
from values_table)
select group_range,
count(v) as cnt_v
from tmp
where group_range is not null
group by group_range
 ```
</details>
</details>



<details>
  <summary>Декабрь 2025 (21 штук)</summary>

<details>
  <summary>Сбер</summary>
  вт

# [1] Задача

 ```sql
--Сколько клиентов видели рекламу не менее 2х раз

select id, count(id) as cnt
from events 
where event = 'SHOWN'
group by id
having count(id) >= 2
 ```

 # [2] Задача

 ```sql
--Сколько было показов и кликов
select
count(case when event = 'SHOWN' then 1 end) as shown,
count(case when event = 'STARTED' then 1 end) as started
from events

-- Конверсия клики/показы
with tmp as (select
count(case when event = 'SHOWN' then 1 end) as shown,
count(case when event = 'STARTED' then 1 end) as started
from events)
select round(started::float/shown,2)*100 as cr
from tmp

-- Сколько было сделано продаж в течение 4х дней после клика?

with tmp as (select id, s.ts as ts_sale, e.ts as ts_click
from sales s left join events e on s.id = e.id and e.event = 'STARTED' and s.ts > e.ts)
select count (distinct id)
from tmp
where datediff('dd', ts_click, ts_sale) <= 4 and ts_click is not null
 ```

 # [3] Кейс АБ

Проведен АБ тест - показаны 2 разных кредита разным группам людей.
Полученные результаты показывают отсутствие разницы между сtr.
Вы сомневаетесь в правильности выводов, поскольку это противоречит ранее проведенным исследованиям, которые проводили не вы.
С чем вы связываете отсутствие различия?

Что нужно проверить?
- корректность проведения аб теста (нет ли peeking prblem, srm, сколько держали аб и набралась ли достаточная выборка, корректно ли выбран стат критерий)
- было ли соблюдено правило случайного распределения пользователей, действительно ли пользователь оказался только в одной группе
- не было ли такого, что тестировали на разных каналах привлечения
- не проводился ли тест в праздники? или других отличные от будней дней?
- не было ли множественного тестирования
- возможно была выбрана некорректная метрика
- баги

 # [4] Кейс

 Ситуация: пришел на работу -> витрина пустая в БД. Что делать?

1) Посмотрю рабочие чаты и почту - вдруг DE уже предупредили о проблеме?
2) Пойду узнаю у ответственных за витрину - если это не моя ответственность
3) Если моя, то пойду проверять, что сломалось:
- проверю логи, алерты, чтобы понять, когда витрина упала
- проверю, когда были сделаны последние изменения
- проверю, все ли окей с источником данных
- если есть бэкап, то временно поднимаю витрину из бэкапа
- запускаю бэктил (перерасчет данных)
- иду к de

Какие вопросы важно задать?

Когда перестала работать? (по логам/мониторингу)
Что изменилось перед этим? (деплой, изменение данных, нагрузки)
Кто использует витрину? (чтобы оценить срочность)
Есть ли бэкап/ snapshot? (можно ли быстро восстановить)

</details>

<details>
  <summary>Сбер Здоровье</summary>

ср

# [1] Задача

 ```sql
-- Вывести разницу income в процентах по каждой стране YoY, сравнивая октябрь 2022 с откябрем 2023

with tmp as (select extract(year from s.date) as yr, country_name, sum(income) as sm
from country c  left join sales s on c.city = s.city and extract(month from s.date) = 10 and extract(year from s.date) in (2022,2023)
group by country_name, yr)
select country_name,
(sm - lag(sm) over (partition by country_name order by yr))/lag(sm) over (partition by country_name order by yr)*100 as perc
from tmp
where yr is not null
 ```

 # [2] Задача

 ```sql
-- Запрос retention-анализа

with first_actions as (select user_id, date_trunc('week', ts) as week, 
date_trunc ('week', min(ts) filter (where event_name = 'Install')) as f_install,
date_trunc ('week', min(ts) filter (where event_name = 'Consultation')) as f_consultation
from event_log
group by user_id, date_trunc('week', ts)),

retention as (
  select f_install, user_id,
  (f_consultation - f_install) / interval '1 week' as diff
  from first_actions
)

select f_install as "Неделя первой установки", 
avg(diff) AS "Количество недель между первой установкой и Consultation",
count(distinct user_id) AS "Количество уников в когорте",
count(distinct case when diff is not null then user_id) AS "Количество уников совершивших Consultation в эту неделю"
from retention
group by f_install
order by f_install
 ```
# [3] Задача

Что делает конструкция df['col'] = df['col'].map(lambda x: 1 if x=='test' else 0)?

к каждому значению в столбце применяется функция лямбда6 то есть если значение в столбце = test, то тогда пишется 1, если не test, то 0

Альтернатива: df = df.apply(lambda x: 1 if x['col']=='test' else 0, 1)?

если значение в столбце = test, то тогда возвращается 1, если нет, то 0 (а к строке, потому что стоит 1 в apply)
вернется не df, а series, то есть [1,0,0,1 и тд]

# [4] Задача
Что делает этот код?
sorted(zip([1230930093, 123630960960, 123960132511], ['Maxim A.', 'Alexander P.', 'Ilya R.']), key=lambda x: x[1], reverse=True)

zip - объединяет списки в кортеж
сортирует кортеж по именам (потому что 1 - индекс) и в обратном алфавитном порядке

# [5] Задача

 ```python
import pandas as pd
df = pd.read_csv('Название')

'''Для каждого магазина вывести общую сумму продаж'''

df['total'] = df['amount'] * df['price']
df.groupby('store')['total'].sum()

'''Для каждого магазина вывести среднюю цену товара'''
df.groupby('store')['price'].mean()

'''Для каждого магазина вывести количество уникальных товаров'''
df.groupby('store')['item'].nunique()

 ```
# [6] Задача

Представим интернет-магазин, в котором есть 2 канала продаж: web и mobile. Во всех каналах вырос средний чек. Что произошло с общим средним чеком?

Нельзя однозначно сказать, что стало с общим средним чеком - он мог как упасть, так и вырасти, так и не измениться. Все дело в парадоксе симпсона.

Приведу пример

Пусть люди покупают компьютеры и телефоны в магазинах А и Б

В магазине А:
Веб: 
телефоны: 15
компьютеры: 18
доля тех, кто купил телефоны: 0,45

Мобайл
телефоны: 18
компьютеры: 9
доля тех, кто купил телефоны: 0,67

В магазине Б:
Веб: 
телефоны: 9
компьютеры: 12
доля тех, кто купил телефоны: 0,43

Мобайл
телефоны: 27
компьютеры: 15
доля тех, кто купил телефоны: 0,64

Мы видим, что доля той категории покупателей, кто купил телефон выше и в магазине А, и в магазине Б 

Теперь объединим

Магазин А: (15+18)/(15+18+18+9) = 0,55
Магазин Б: (9+27)/(9+12+27+15) = 0,57

И вот тут парадокс Симпсона! При объединении категорий получили, что доля выше в магазине Б

# [7] Задача

Объясните, что такое ошибка первого рода (α) и второго рода (β) в контексте проверки гипотез.
Как они связаны с нулевой и альтернативной гипотезами?

ошибка первого рода - это вероятность отвергнуть нулевую гипотезу, при условии, что она верна (FPR)
в контексте проверки гипотез берут критерий значимости альфа - это и есть вероятность "ошибиться", то есть ложно отвергнуть нулевую гипотезу

ошибка второго рода - это вероятность не отклонить нулевую гипотезу, при условии, что она не верна (FNR)
также есть мощность теста (1 - бетта) - это вероятность правильно отклонить нулевую гипотезу когда h1 верна

</details>

<details>
  <summary>Medsi всякие теор вопросы</summary>
  чт

# [1] BI / SQL / Data Engineering

• Опиши полный процесс разработки дашборда: сбор требований, формализация
метрик, проверка наличия данных, создание витрин, подключение к Power BI,
публикация, документация

- сначала необходимо понять бизнес-задачу, цели, какие проблемы будет решать, собрать требования к дашборду
- затем создание прототипа, метрик
- подготовка и проверка данных, создание витрины
- подключение к powerbi
- создание дашборда
- публикация
- документация
- инструктаж

• Как устроена структура DAG в Airflow? Как формируются SQL-скрипты, где
хранятся, как выполняются?

- DAG в Airflow позволяют выполнять определенный скрипт по расписанию - например, заливать данные в витрину

DAG состоит из тасков(operators), которые могут быть sql-скриптами, python-запросами и тд
SQL-скрипты обычно хранятся отдельно в репозитории и выполняются через оператор

• Как работает импорт данных в Power BI: отличие Import Mode от DirectQuery?
В PowerBI используется схема звездочка, то есть связь один ко многим, поэтому можно работать с данными прямо в PowerBI. Можно импортировать из
разных источников - БД, файлы, эксель и тд. Import Mode - данные импортируются в PowerBI и расчеты происходят внутри повербиай. 
DirectQuery - повербиай отправляет запросы в БД, поэтому данные скорее всего актуальные 

• Как ты создаёшь витрину в ClickHouse и запускаешь её в автоматический
пересчёт через Airflow?
1) Я делаю скл-скрипт, который джойнит и фильтрует данных (через DBeaver) и формирую таблицы в dwh. 
2) Проверяю скрипт локально в дбивере
3) Загружаю скрипт в репозиторий или хранилище файлов
4) Далее пишу скрипт dag, где один из тасков выполняет sql-скрипт на clickhouse через clickhouseoperator
5) настраиваю расписание DAG, чтобы данные обновлялись
6) Проверяю выполнение и мониторю DAG через веб-интерфейс Airflow

# [2] A/B-тестирование

• Как формулируется гипотеза, как выбираются метрики (целевые и защитные)?

- гипотеза формулируется исходя из задачи бизнеса и возможно какого-то проведенного исследования, а возможно просто от желания протестировать новую фичу
- метрики выбираются исходя из того, что мы хотим проверить (конверсия, средний чек?), при этом метрика должна быть устоячива к шуму (низкодисперстной), но при этом и быть детектируемой за небольшой период времени (то есть не ltv). также метрика должна давать четкий ответ на вопрос - стало лучше/хуже (например, конверсия выросла - это хорошо), метрика должна отражать эффект от этой фичи (например, лучше проверить конверсию в нажатие кнопки от нового попапа, чем проверять конверсию в продажу от всего лендинга), также метрика должна быть такой, чтобы ее могли задетектить стат тесты, также метрика должна быть косвенно или напрямую связана с прибылью
гипотеза должна быть конкретной, направленной и измеряемой количественно

- защитные метрики должны отражать какие-то стороны продукта, которые мы не должны поломать новой фичей - например, конверсию в оплату или время проведенное на сайте. то есть эта метрика тоже должна быть четкая, умеренно чувствительная и иметь понятное бизнесовое значение (почему ее ломать - это критично?). защитные метрики должны отражать риски

• Как рассчитывается MDE, какое количество наблюдений нужно, сколько
должен идти тест?
- MDE рассчитываем, исходя из мощности теста, альфа, кол-во наблюдений, длительности теста - мы подставляем параметры и получаем определенный mde. если мы понимаем, что этот mde нам достигнуть почти нереально, то нужно менять что-то в параметрах - например, увеличивать выборку или срок теста

- также и с остальными - кол-вом наблюдений и сроком теста. мы можем заранее понять (посчитать юнит экономику), какой нам нужен mde, и уже отталкиваюсь от него считать срок и кол-во наблюдений

1) бизнес говорит mde
2) аналитик считает выборку
3) выборка определяет длительность теста
4) если длительность большая, меняем параметры

• Как выбирается статистический критерий (t-test, z-test, U-test, bootstrap)?
- выбирается в зависимости от метрики (конверсионная, числовая и так далее), то есть выбирается от распределения, объёма данных, дисперсии

• Как проектируется рандомизация без сплитовалки? Какие атрибуты
балансировать?
- используем хэш-функцию
нужно балансировать (гео, канал трафика, тип устройства, платформа, новая/повторная сессия, объём трафика)

• Как интерпретировать доверительный интервал?
- мы получаем определенное значение после эксперимента, далее смотрим доверительный интервал - с вероятностью (1 - альфа) наше истинное значение будет лежать в этом интервале при проведении множества экспериментов
мы можем получить стат значимый результат, но в доверительный интервал попадет значение, которое нам будет не особо полезно как бизнесу (маленькое), тогда мы задумаемся, а стоит ли вообще катить фичу - то етсь мы не полагаемся только на p value

• Как проводить мультивариантное тестирование и учитывать поправку
Бонферрони?

- если у нас несколько сравнений (множественное стравнение), то тогда мы можем поделить на кол-во гипотез, чтобы уменьшить ошибку первого рода


# [4] Юнит-экономика / Метрики

• Как формировать финансовую модель продукта?

финансовая модель продукта включает в себя:
- прогноз трафика, конверсий, то есть воронки
- юнит-экономику (cac, ltv, arpu, arppu, roi, cogs)
- модель выручки(подписка, комиссии, GMV - revenue) и затрат (прямые/косвенные/переменные/постоянные)
- P&L по месяцам (выручка, EBITDA)
- кэшфлоу (движение денег во времени)
- точку безубыточности

• Как распределять доходы/расходы по периодам?
если нет подписочной модели, то тогда какие в этом месяце доходы и расходы, то и относим в данный месяц
что касается косвенных расходов, то есть тех, которые не связаны напрямую с продуктом, то лучше их либо делить на 12, либо уже включать только в годовую отчетность

если подписочная модель, то тогда записывать не всю оплату в один месяц, а растягивать на весь срок подписки, чтобы не забирать деньги у себя в будушем

• Что является юнитом?
- минимальная сущность, на которой можно посчитать маржу, доходы, расходы

• Как учитывать прямые и косвенные расходы?
- прямые учитываем в экономике продукта, косвенные только в отчетности, но не в юнит экономике

• Как считать LTV и отток, почему нельзя использовать фиксированное окно?
LTV считаем как (arpu*валовая маржа)/churn
отток = 1-retention

фиксированное окно использоватьт нельзя, потому что retention меняется, также по когортам ltv меняется

</details>


<details>
  <summary>Lofty</summary>
  пт

# [1] SQL

 ```sql
-- Требуется: написать SQL‑запрос, который вернёт всех пользователей из таблицы users, которые никогда не оставляли комментарии (то есть их user_id не встречается в таблице comments)

select user_id
from users left join comments using (user_id)
where comment_id is null
 ```

 # [2] Статистика и A/B‑тесты

 1) Задача 1. Тест для метрики‑отношения (конверсии)

 Компания проводит A/B‑тест, целевая метрика — конверсия (отношение количества целевых действий к числу пользователей или показов). Нужно:

описать, какой статистический тест уместно использовать для сравнения конверсий между группами;

z-тест для пропорций, t-тест применим только при линеаризации

объяснить, зачем применяется техника линеаризации и как её использовать вместе с выбранным тестом.
линерализация - техника преобразования метрики, которая уменьшает дисперсию и позволяет использовать t-тест



2) Задача 2. Интерпретация p‑value

Объясните, что такое p‑value в контексте A/B‑тестирования и проверки гипотез:

как его правильно интерпретировать;

p - value - вероятность получить такое или более экстремальное значение при условии, что нулевая гипотеза верна
если p-value < a, то тогда мы отклоняем нулевую гипотезу и после построения дов интервала понимаем, есть ли полезный эффект от фичи

как оно связано с уровнем значимости α и ошибкой первого рода;

мы сравниваем p-value с уровнем значимости и принимаем решение на основе этого - по сути чем больше ошибка первого рода (альфа), тем чаще p-value будет отклонять гипотезу

какое решение мы принимаем в зависимости от величины p‑value.

есть ли стат значимый эффект или нет - катим фичу или нет



3) Задача 3. Стратификация в A/B‑тестах
Нужно:

объяснить, что такое стратификация в A/B‑тестировании;

- это разбиение трафика на однородные группы (страты) перед рандомизацией, чтобы внутри каждой группы пропорции АБ были одинаковыми 

как формируются страты и по каким признакам это можно делать;

- устройство, гео, новый/старый пользователь, канал трафика 


каким образом стратификация влияет на дисперсию оценок и точность результатов теста.

- она уменьшает ошибку первого и второго рода, поскольку мы можем либо задетектить эффект, где его нет, либо наоборот не увидеть, поскольку какая-то группа будет оказывать больше воздействия - и тогда не соблюдется условие случайного распределения пользователей

(то есть уменьшает дисперсия оценки эффекта)


4) Задача 4. Проверки перед запуском A/B‑теста
Перед запуском A/B‑теста команда хочет убедиться, что тест корректно задизайнен и будет давать достоверные результаты. Опишите:

какие проверки и sanity‑чекы нужно сделать до старта теста (или на этапе A/A‑теста);

- сначала прогоняем хэщ-функцию на большом кол-во юзеров (то есть симулируем распределение)
- потом проверяем равномерность распределения (кол-во в каждой группе, разбиение по сегментам)
- попадает ли пользователь всегда в одну и ту же группу
- провести а/а тест и проверить, не выдает ли наша метрика p-value ниже альфы
- можно с помощью монте-карло просимулировать аа-тест (чтобы проверить корректную работу критерия - точно ли у нас получится уровень значимости a), также можес просимулирвоать аб тест на исторических данных, чтобы сравнить тесты и понять, у какого мощность выше

какие распределения и метрики стоит сравнить между группами; 

- распределение по сегментам (гео, новый/старый, пол, возраст, канал трафика)
- конверсию в целевое действие, arpu, размеры сессий и тд

что является «хорошим» результатом таких проверок;

- что p-value>0.05 в аа тесте
- распределение по сегментам примерно одинаковое
- кол-во пользователей в каждой группе одинаково

какие выводы можно сделать, если обнаружены сильные перекосы между группами.

- что хэш-функция работает некорректно, возможно стоит применить стратификацию
- тест запускать нельзя


5) Задача 5. Оценка эффекта без A/B‑теста
Фича уже целиком выкатана на продакшен, провести классический A/B‑тест невозможно. При этом нужно оценить её влияние на ключевые метрики продукта.

Вопрос: какие подходы можно использовать для оценки эффекта в такой ситуации (до/после, квази‑эксперименты и т.п.), и какие ограничения у этих подходов?

- casual inference (нужно много признаков для корректного моделирования)
- diff and diff (не всегда можно найти схожие группы пользователей, эффект фичи может смешиваться с другими эффектами, нужен тест и контроль)
- временные ряды (нельзя отделить эффект фичи от других изменений в продукте или сезонности)

 # [3] Python / Pandas

 Задача 1. Объединение двух DataFrame

Имеются два объекта pandas.DataFrame, в обоих содержится общий ключевой столбец user_id. Опишите, как в Pandas можно:

выполнить объединение по ключу (аналог SQL JOIN);

- merge

какие виды соединений поддерживаются (left, right, inner, outer);

left, right, inner, outer

какими методами библиотеки pandas это реализуется.

df.merge (df1, on = '', how = '')


Задача 2. Фильтрация по дате
Дан DataFrame df с колонкой event_date в формате даты. Нужно отфильтровать строки только за одну конкретную дату, например '2024-01-01'.

```python
df = df[df['event_date'] == '2024-01-01']
```
Опишите, как это сделать с помощью булевой индексации и/или методов .loc в Pandas.

```python
df = df.loc[df['event_date'] == '2024-01-01']
```
# [4] Продуктовые и бизнес‑кейсы

# [4.1] Кейс 1. Сравнение CPC и CPM‑кампаний при выборе победителя аукциона

RTB‑платформа одновременно обслуживает рекламные кампании двух типов:

CPC — рекламодатель платит только за клики;
CPM — рекламодатель платит за показы (за 1000 показов).
Во время аукциона для одного рекламного слота одновременно есть несколько кандидатов‑баннеров с разными ставками и разным типом оплаты. Для каждого баннера есть исторические данные и/или предсказанная вероятность клика (CTR).

Требуется: предложить, как сравнивать между собой ставку за показ (CPM) и ставку за клик (CPC), чтобы выбрать для показа наиболее выгодный для платформы баннер. Какие метрики или ожидаемые величины дохода нужно считать и по какой формуле?

1) Что нам вообще нужно сделать?

Нам нужно заработать как можно больше денег, выбрав правильный баннер (то есть разница прибыли и затрат должна быть максимальной). Для платформы чем лучше алгоритм, тем больше прибыли от баннера как платформе, так и рекламодателю. И к тому же, нам нужно сделать так, чтобы мы не потратили бюджет рекламодателя впустую


2) Определеяем ожидаемый доход с каждой ставки

CPCод = CPCставка * 1000 показов * CTR (чтобы сделать эквивалентным CPM)
CPMод = CPMставка (доход уже за 1000 показов)

3) Сравниваем баннеры:
Выбираем CPCставка * 1000 показов * CTR  или CPMставка (что больше - то и выбираем)

# [4.2] Кейс 2. Сегментация рекламодателей

Задача: провести сегментацию рекламодателей платформы. В вашем распоряжении есть данные о каждом показе и клике, в том числе:

идентификатор рекламодателя;
баннер и формат;
стоимость показа/клика;
частота показов и кликов;
общий бюджет и траты рекламодателя в платформе;
дополнительные поведенческие метрики.
Вопросы:

какие признаки (фичи) вы бы использовали для сегментации рекламодателей;

если более классически, то разбила бы работодателей по:
- сумме ставок (низкорискованые, среднерискованые и высокорискованные) - признак стоимость показа/клика
- и также по платежеспособности работодателя (общий бюджет и траты рекламодателя в платформе)
- по форматам (классификаиция)
- по частота показов и кликов

если с использованием ML, то можно сделать классификацию (kmeans, например) и посмотреть shap - то есть какие признаки на какой кластер повлияли - здась уже получатся новые интересные зависимости

какие типы сегментов можно построить (по бюджету, качеству трафика, стабильности, вовлечённости и т.п.);

- написала выше

какие практические действия можно предпринять для разных сегментов (условия, офферы, приоритизация поддержки и т.д.).

- крупные рекламодатели - персональный менеджер, скидки за объём, приоритизация поддержки
- малые рекламодатели - образовательные материалы, рекомендации по оптимизации
- высокая вовлеченность - предложить новые форматы, бета-функции, апселл
- низкая вовлеченность - уведомления о промо, автоматизированные офферы

</details>

<details>
  <summary>Pari Дофига кейсов</summary>
  сб

# [1] Парадокс Симпсона и ARPU по регионам

> Условие. Есть коммерческая компания, работающая в разных регионах России. Все регионы делят на два кластера:
> Кластер A: Москва + Санкт-Петербург.
> Кластер B: все остальные регионы.
> Год к году средний ARPU вырос в кластере A и вырос в кластере B. Однако по всей стране в целом (A + B) ARPU год к году снизился.

Вопросы:

**Может ли такая ситуация быть в реальности? Если да — почему?**

Да, может, об этом говорит парадокс Сипмсона. Например, изменились размеры кластеров, а соответственно и их веса в общей формуле. Изменение в структуре выборки могло перебить рост внутри кластера

**Приведи интуитивное объяснение (без формул) и конкретный пример, в чём здесь «подвох».**

Например, в кластере А ARPU высокий, в кластере В низкий. Если резко увеличивается доля в В (при том, что рост ARPU вырос, но остается ниже, чем в А), то вклад дешевого сегмента будет увеличиваться сильнее, чем рост арпу

Конкретный пример

2024
Кластер А: 1000
Кластер В: 200
Средний арпу: 1000 * 0.1 + 200 * 0.9 = 280
Доля жителей: кластер А - 10%, кластер В - 90%

2025
Кластер А: 1100
Кластер В: 250
Средний арпу: 1100 * 0.03 + 250 * 0.97 = 275.5
Доля жителей: кластер А - 3%, кластер В - 97%

В каждом кластере арпу вырос, а средний упал

**Как бы ты проверял подобные эффекты в реальных данных, чтобы не попасть в ловушку агрегации?**

* Нужно смотреть не только метрику в общем, но и по сегментам, нельзя делать однозначный вывод о конкретном сегменте, опираясь на данные за все сегменты
* Пересчитать арп за текущий год со структурой прошлого

# [2] Новый алгоритм поиска в e-commerce: дизайн A/B-теста

> Условие. В интернет-магазине есть текущий алгоритм поиска (baseline) и новый алгоритм.
> По offline-метрике (оценка релевантности асессорами) новый алгоритм работает лучше baseline.
> Нужно понять, даёт ли новый поиск реальный прирост в деньгах и пользовательских метриках online.

Вопросы:

**Как ты задизайнишь online A/B-тест для проверки нового поиска?**

Поскольку здесь фича уже раскатана, то мы не можем применить стандартный т тест, для этого мы можем использовать
- козал инференс (моделируем на исторических данных, как бы выглядели метрики, если бы фича не была раскатана и сравниваем с тем, как ведут себя метрики в реальности)
- див ин диф (сравниваем с метриками или с группой, с которой есть корреляция - нсколько изменилась разница в метриках?)
- временной ряд (моделируем только одну метрику и сравниваем есть ли разница с реальностью)

Но будем считать что фича не раскатана на всех, а мы только планируем это делать, поэтому будет проверять гипотезу о равенстве средних через т-тест

H0: мат ожидания равны
Н1: мат ожидание нового алгоритма выше

Метрики: 
Целевые:
CTR (из поиска в первые три карточки)
CTR (из веедния запроса в добавление в корзину)
Revenue per search
Search conversion rate

Заградительные:
% churn (кто зашел, ввел запрос и ушел)

Информационные:
Скорость алгоритма


поскольку это ratio метрика, то нам нельзя использовать т тест, поэтому используем бусттрап/линеаризацию

**На что смотреть, чтобы не «каннибализировать» другие зоны сайта (главная, рекомендации и т.п.)?**

Можно смотреть на CTR в рекомендательных блоках и на главной, изменение распределения заходов на страницы поиска и другие 

- Как ты будешь считать объём выборки: какую MDE возьмёшь, какие значения α и β (мощность) заложишь, и как изменение мощности (80% → 90%) повлияет на требуемый размер выборки?

a - 5%
b - 80% (по стандарту)

MDE посчитаю из юнит экономмики и параметров теста
объём выборки посчитаю через ноутбук по функции из расчета MDE, a, b, дисперсии
увеличение мощности повлияет на больший размер выборки

# [3] Денежные метрики: t-test, распределения и снижение дисперсии

> Условие. В A/B-тестах используются денежные метрики:

>ARPU (средний доход на пользователя).
>Средний чек.
>Количество заказов на пользователя.


**1 - Какими статистическими тестами ты бы проверял значимость разницы по этим метрикам в A/B-тесте?**

ARPU - использовать уровень пользователя (то есть распределение состоит из общей суммы ставок на 1 пользователя), использовать Welch’s t-test (модификация т-теста, не требует равенства дисперсий)
AOV - бутстрап/ленеаризация + t-test
Количество заказов на пользователя - т - тест на уровне пользователя / модель отрицательной биномиальной регрессии
**2 - Почему наивный t-test может плохо работать для ratio-метрик (например, среднего чека) и тяжёлых хвостов распределения выручки?**

Потому что и при ARPU, и при AOV мы получаем сильно скошенные распределения, поскольку в ratio метриках зависимость числителя и знаменателя приводит к сложной дисперсии, поэтому при малых заказах метрика сильно гуляет. С тяжелыми хвостами т-тест может справляться, поскольку т-тест чувствителен к сильным выбросам

**Как выглядит распределение ARPU в реальном продукте и какие проблемы оно создаёт для тестов?**

Это экспоненциальное распределение, большинство значений сосредоточено в левой части графика. Для тестов может создавать проблему в том, что довольно большая дисперсия, поэтому тест длится дольше, чтобы набрать достаточную выборку

**Какие подходы к снижению дисперсии ты знаешь и как их применил бы здесь (CUPED / ковыриаты, стратификация / пост-стратификация, обрезка/винсоризация хвостов и т.п.)?**

1) CUPED - выбираем коррелирующую метрику, проводим тест, строим линейную регрессию/линеаризацию (для ratio) и получаем очищенные значения метрик (то есть CUPED убирает шум)

2) Ковариаты - берем уже несколько коррелирующих метрик и строим регрессию с несколькими параметрами

3) Стратификация - сначала делим пользователей на группы, а потом внутри каждой группы случайно делим на а и б (используется для сильно различающихся групп)

4) Пост-стратификация - делаем так уже после проведения теста

5) Винсоризация - когда мы обезаем выбросы (например, по 2,5% с каждой стороны)

В каких случаях CUPED и подобные методы будут работать плохо или почти не дадут выигрыша?

Когда CUPED и подобные методы плохо работают:

Метрика мало коррелирует с предтестовыми данными → эффект минимален
Очень шумные или редкие события (например, покупки у небольшой доли пользователей)
Если ковариата не отражает вариацию текущей метрики

**Что делать с heavy users / «китами», которые могут сильно «ломать» денежные метрики и результаты теста?**

Можем их исключить их теста и проводить только на бОльшей части пользователей или винзоризация


# [4] Два изменения в пуш-рассылке: как разделить эффекты?

> Условие. В букмекерской компании запустили CRM-рассылку:

> В тексте пуша/сообщения изменили формулировку (коммуникацию).
> Одновременно изменили размер фрибета (сам оффер).
> Рассылку уже отправили, но теперь бизнес хочет понять: отдельный эффект текста и отдельный эффект изменения фрибета.

**1 - Как бы ты перезадизайнил эксперимент, чтобы разделить влияние текста и оффера?**

Можно разделить пользователей на 4 группы:
A - те, кто получат старый пуш
В - те, кто получат пуш с новой коммуникацией
С - те, кто получат пуш с новым оффером
D - те, кто получат пуш и с новым оффером, и с новой коммуникацией

Также нужно использовать поправку Бонферрони, чтобы протестировать сразу несколько гипотез

**Какой дизайн теста выбрал бы?**

факторный дизайн с несколькими группами

**Как учитывать проблему множественных проверок и контролировать общий уровень ошибок (подходы типа Bonferroni, Holm и др.)?**

Поправка Бонферрони или Холма

**Какие целевые метрики смотришь (открытие, переход, депозит/ставка, ARPU, отписки и т.д.)?**

Целевые:
CTR в открытие
CTR в выполнение целевого действия

Заградительные:
% тех, кто отключил уведомления после получения пуша
ARPU

# [5] Инкрементальный эффект продукта «боевой пропуск» (battle pass)

> Условие. В продукте есть «боевой пропуск»/прогресс-бар:

> Игроки выполняют игровые и ставочные задания, получают внутреннюю валюту и награды (фрибеты, скины и т.п.).
> Есть дополнительные расходы бизнеса на эти награды и на разработку.
> Классический A/B-тест на включение/выключение этого продукта запустить нельзя.

**1 - Как оценить инкрементальный эффект этого продукта в деньгах?**

- Did - сравниваем изменение выручки у тех, кто включил battle pass, а кто нет

**2 - Как бы ты строил тест/квази-эксперимент: какие группы использовать, как подбирать «двойников», по каким признакам матчить игроков?**

Для матчирования можно взять:
- исторический ARPU
- частота ставок
- типы ставок
- устройство, страна, канал привлечения
- ретеншн

test - те, кто включил battle
control - двойники, которые не включили

Методы матчирования:
- Логистическая регрессия
- propensity score matching

**3 - Как проверять, что до запуска/включения боевого пропуска тестовая и контрольная группы действительно сопоставимы по ключевым метрикам (например, депозиты, выручка, активность)?**

Можно посмотреть на временные ряды метрик - насколько у них коррелируют спады/падения? ну и на исторических данных посмотреть корреляцию
также можно проверить стат значимость по сопоставимым метрикам (например, по т-тесту и хи-квадрату)

**4 - Какой горизонт эффекта и какие метрики (ARPU, частота ставок, удержание) ты бы анализировал?**

Я бы взяла горизонт в минимум 2 недели, чтобы учитывать сезонность, но и учитывала бы срок battle . Смотрела бы ARPU uplift, частота ставок, Retention

# [6] Прогноз продаж и отклонение факта: кейс Ламоды

>Условие. Есть прогноз продаж крупного интернет-ритейлера (пример: Lamoda):
>Модель учитывает тренд, сезонность и какие-то внешние факторы.
>В одном из месяцев (октябрь) факт оказался на ~15% ниже прогноза.

**1 - Как понять, является ли такое отклонение аномальным с точки зрения модели (доверительный интервал прогноза)?**

- построить Prediction Interval (PI) и проверить находятся ли эти 15% внутри него

**2 - Как ты будешь искать причины отклонения: по каким разрезам и факторам пойдёшь?**

- посмотрю на исторические данные
- посмотрю по когортам - может, выручка где-то в одном месте подскочила
(категории товаров, регионы, тип устройств, каналы привлечения)
- посмотреть воронку - может, где-то конкретно падение?
- посмотреть метрики по доставке - может, тут что-то пошло не так?

**3 - На какие внутренние (цены, промо, ассортимент, технические проблемы) и внешние факторы (погода, конкуренты) посмотришь?**

внутренние:
- поломки в данных
- отключили рекламный канал/урезали бюджет
- запустили новый продукт/линий/монетизацию
- техбаги
- out of stock
- изменения в приложении, новый функционал, дизайн

внешние:
- экономическая ситуация (было ли что-то особенное в октябре?)
- конкуренты (может, перешли к конкурентам)
- аномалии в погоде (например6 аномально теплая погода или наоборот заморозки)

Как по результатам анализа скорректировать модель/процесс прогнозирования?

- если это аномалия, то добавить anomaly flag и не включать месяц в обучение
- если нет, то добавить новые признаки и обновить модель

# [7] Общий инкрементальный эффект CRM-коммуникаций

> Условие. В компании много CRM-каналов и активностей:
> Пуш-уведомления, SMS, email-рассылки.
> Коммуникации с матчами, промо, персональными офферами и т.п.
> Команда видит по операционным метрикам, что какие-то кампании «стреляют», у каких-то высокие открытия и клики, но нет понятного ответа: сколько в целом денег приносит направление CRM инкрементально.

**1 - Как оценить общий инкрементальный эффект CRM-коммуникаций, а не только «последний клик» от конкретного пуша?**

- временно отключить некоторые каналы и посмотреть, что изменилось (сильно ли снизилась выручка?)
- посмотреть корреляцию кол-ва взаимодействий с пользователями и выручкой по дням 

**2 - Как можно использовать глобальные контрольные группы (часть базы, которую на время вообще не трогаем коммуникациями) для оценки эффекта?**

мы можем в тестовую группу определить тех, кто не будет получать коммуникацию, а в качестве контрольной взять тех, кто получает коммуникацию

**3 - Какие риски и ограничения есть у такого подхода и как их минимизировать (выбор размера и состава контроля, длительность эксперимента, этика/бизнес-риски и т.п.)?**

риски и ограничения:
- потеря выручки (для минимизации можно отключать каналы постепенно, начать с того, который по прогнозам приносит меньше всего прибыли)
- часть пользователей отвалится, потому что не будет получать коммуникацию, то есть мы можем понизить ретеншн (не отключать критически важные коммуникации)

**4 - Какие метрики на горизонте эксперимента ты бы анализировал (выручка, депозиты, удержание, LTV, отписки, усталость от коммуникаций)?**

- Revenue per channel
- LTV
- Retention
- Отписки

</details>

<details>
  <summary>AliExpress</summary>
вс

# [1] Задача

 ```sql
select t1.fullname, t1.salary, t2.fullname
from employees t1 join employees t2 on t1.managerid = t2.id and t1.salary > t2.salary
 ```

 # [2] Задача

 ```sql
with tmp as (select user_id, order_dt,
sum(gmv) over (partition by user_id order by order_dt) as sm
from orders
where extract (year from order_dt) = 2025),

  tmp_2 as (select user_id, order_dt, sm,
row_number() over (partition by user_id order by order_dt) as rn
from tmp
where sm >= 10000)

select user_id, order_dt
from tmp_2
where rn = 1
 ```

# [3] Задача

 ```sql
with tmp as (
  select user_id, name, count(order_id) as cnt
  from users u join orders o on u.user_id = o.user_id 
  where o.status = 'done'
  group by user_id, name
)
select name 
from tmp
where cnt >= (select percentile_cont(0.95) within group (order by cnt) from tmp)
 ```

 # [4] Задача

 ```python
 def func(nums, target):
  final = []
  nums.sort()
  for i in range (len(nums)):
    if nums[i] == target:
      final.append(i)
  
  if final == []:
    return [-1, -1]
  else:
    return [final[0], final[len(final)-1]]
 ```
</details>

<details>
  <summary>Tripster</summary>
пн

# [1] Задача SQL

 ```sql
with tmp as (select user_id, 
min (case when event_time = 'install' then event_time ens) as first_install,
min (case when event_time = 'purchase' then event_time ens) as first_purchase
from events
group by user_id)

select user_id, DATEDIFF ('dd', first_install, first_purchase) as day_diff,
revenue
from tmp t1 join events t2 
on t1.user_id = t2.user_id 
and t1.first_purchase = t2.event_time
and t2.event_type = 'purchase'
 ```

# [2] Задача SQL

 ```sql

with tmp as (select user_id, event_time, 
count(*) over (partition by user_id) as cnt_pur,
lag(event_time) over (partition by user_id order by event_time) as lg_pur
from events
where event_type = 'purchase')

tmp_2 as (
select user_id, max(datediff ('dd', lg_pur, event_time)) as max_gap_days
from tmp
where cnt_pur >= 3
group by user_id)

select user_id,
case
when max_gap_days > 30 then True else False
end as churned
from tmp_2
 ```

# [3] Python - Построение таблицы ретеншена пользователей по дням

 ```python
'''1 - Сначала найдем когорты'''
install_days = df[df['event_type'] == 'install'][['user_id', 'event_date']].rename(columns = {'event_date':'install_date'})

'''2 - Теперь отделим сессии пользователей'''
sessions = df[df['event_type'] == 'session'][['user_id', 'event_date']].rename(columns = {'event_date':'session_date'})

'''3 - Соединяем сессии и дни установки, чтобы посчитать разницу между сессией и установкой для каждого пользователя'''
sessions = sessions.merge(install_days, on = 'user_id')
sessions['diff'] = (sessions['session_date'] - sessions['install_date']).dt.days.rename('diff')

'''4 - Теперь посчитаем, сколько уникальных пользователей в каждой когорте и разнице'''
sessions = sessions.groupby(['install_date', 'diff'])['user_id'].nunique().rename('count_users').reset_index()

'''5 - Теперь сделаем разницу столбцами'''
retention = sessions.pivot(index = 'install_date', columns = 'diff', values = 'count_users')
retention.columns = [f"day_{c}" for c in retention.columns]

'''6 - Получили матрицу ретеншна, теперь сделаем доли'''
cohort_size = install_days.groupby('install_date')['user_id'].nunique().rename('count_cohort')
retention = retention.divide(cohort_size, axis = 0).round(2).fillna(0)
 ```

 # [4] Вопросы по статистике

**Ошибка I рода и ошибка II рода**
1 рода - это вероятность ошибиться ложноположительно, то есть отвергнуть нулевую гипотезу, а она на самом деле верна
2 рода - вероятность ошибиться ложноотрицатльно, то есть принять нулевую гипотезу, а она не верна

**p-value**

это вероятность увидеть такое или более экстремальное значение при условии, что нулевая гипотеза верна

**Доверительный интервал**

это диапазон, который с заданной вероятностью содержит истинное значение

**Перцентили**

это значение, ниже которого находится заданный процент значений

# [5] Кейс 1 — «Баннер и сравнение кликнувших vs некликнувших»

> Контекст:
> У компании есть сайт-маркетплейс. На главной странице был выкатан новый баннер. При клике по баннеру пользователю показывается анимация и сообщение «Вы наш лучший клиент». Больше ничего функционально не меняется.

> Менеджер не запускал A/B-тест, он просто выкатил баннер на всех. После выката он посмотрел метрики: среди пользователей, которые кликнули по баннеру, конверсия в заказ на 50% выше, чем среди пользователей, которые не кликнули по баннеру.

> Менеджер сделал вывод: баннер повышает конверсию → надо поставить его на другие страницы. Он утверждает, что фактически у него получился "псевдо A/B-тест", потому что есть две группы («кликнувшие» и «некликнувшие»), и метрики отличаются значительно и статистически значимо.

**Корректна ли логика менеджера? Можно ли считать, что сравнение кликнувших и некликнувших — это A/B-тест?**

нет, поскольку менеджер упускает наличия третьего фактора - например, что если пользователь активный, то он нажмет на баннер, ну а у активных пользователь конверсия выше. то есть есть некий третий фактор, который коррелирует с другими

ну и для а/б теста была нарушена методология - не был расчет длительности, выборки, MDE, чтобы корректно посчитать стат значимость

**Является ли наблюдаемая разница в конверсии доказательством эффективности баннера?**
**Если нет — почему?**

объяснила выше

**Как нужно было правильно ставить эксперимент?**

сначала нужно было разделить выборку на А и Б, посчитать объём выборки, MDE, установить альфа и бетта, затем выбрать тест (для конверсии - линеаризация + т-тест), потом подождать длительность и далее уже посмотреть на p-value

**Как формировать выборки?**

50/50 - те, кто увидели и не увидели баннер
единица рандомизации - пользователь

**Какие группы нужны?**

**Какие метрики?**

Целевые: 
CTR в целевое действие
ARPU

Заградительные
churn rate

Информационные:
CTR в клики на баннер

**Какие опасности?**

Пользователи, которые увидели этот баннер, могли зачерниться

**Какие данные нужны, чтобы корректно оценить эффект, если A/B-теста не было?**

исторические данные, чтобы проводить casual inference, diff in diff, анализ временных рядов
на этих данных нужно смотреть на корреляции с другими метриками, группами пользователей

# [6] Кейс 2 — «Тип обращения в SMS-рассылках»

>Контекст:
>Компания отправляет пользователям SMS-сообщения. Есть два вида сообщений:

>Тип 1: «Уважаемый клиент»
>Тип 2: «Имя Отчество»
>Фактически уже была проведена отправка сообщений без полноценного A/B-теста:
>Сообщения типа 1 отправили → конверсия 40%
>Сообщения типа 2 отправили → конверсия 45%

>Размеры групп разные:
>Группа 1: 1000 пользователей
>Группа 2: 2000 пользователей

>Менеджеры хотят сделать вывод: Тип 2 лучше, давайте всегда отправлять его.
>Тест был остановлен раньше, чем требовалось. Для MDE = 2% и α = 5% по расчётам требуется 5000+ пользователей в каждой группе, а сейчас в тесте лишь 1000 и 2000.

**Можно ли считать разницу (40% vs 45%) статистически значимой? Что нужно для этого посчитать?**

Нет, потому что нужно смотреть не по процентам, а по p-value/доверительному интервалу

**Достаточно ли объёма выборки?**

Тест некорректный, то есть объёмы выборок разные в разных группах. Тем более по расчету требуется гораздо больше объём, а тест был остановлен раньше

**Как определить, сколько наблюдений надо?**

Нужно посчитать объём в зависимости от MDE, a, мощности, дисперсии

**Что такое MDE?**

Минимально детектируемый эффект, который тест сможет обнаружить при заданном уровне мощности

**Почему с малым количеством данных нельзя делать выводы?**

Потому что при малом объёме выборки высокая случайная вариативность и недостаточная мощность теста, поэтому наблюдаемая разница может быть результатом шума, а не случайного эффекта

**Что делать, если тест был остановлен слишком рано?**

Запускать занаво, либо перезапускать, потому что нельзя делать выводы

**Можно ли использовать группы разного размера?**

Да, группы разного размера допустим, но при сильном дисбалансе снижается стат мощность

**Смущает ли, что группы не 50/50, а 33/67?**

Критично, если SRM

**Нужно ли останавливать тест из-за "слишком большой разницы на первой неделе"?**

Нет, потому что потом всё может выровняться, просто ещё недостаточно наблюдений (peeping problem)

</details>

<details>
  <summary>Гемблинг кейсы</summary>
вт

# [1] Кейс 1. A/B‑тест: разные результаты по гео

>Условие:
>Компания использует единую версию продукта для нескольких стран (единую воронку, UI, флоу).
>Запущен A/B‑тест новой фичи (вариант B).

>По итогам:
>В одном гео — статистически значимый рост ключевой метрики.
>В другом гео — статистически значимое падение.
>В остальных гео — незначимые результаты.

**Что делать продуктовой команде: катить фичу частично / полностью / откатывать?**

Решение о раскатке принимать нельзя без доп анализа. Противоположный эффект на гео указывают либо на гетерогенный эффект (влияние фичи не одинаково для всех пользователей), либо на проблему дизайна теста. Сначала нужно проверить корректность эксперимента, а затем решить - возможна ли частичная раскатка по гео или требуется доработка фичи

**Какие дополнительные проверки нужны?**

- Проверка, нет ли SRM
- Нет ли сетевого эффекта
- Соблюдается ли правильная рандомизация внутри каждого гео
- Causal Random Forest для проверки гетерогенного эффекта

**Какие гипотезы поведения пользователей возможны?**

- Разные пользовательские сценарии и JTBD по гео
- Фича конфликтует с локальными паттернами поведения
- Разные устройства/скорость интернета 
- Проблемы локализации/языка/тех проблемы

# [2] Кейс 2. Анализ воронки верификации (KYC)

>Ты присоединился к продуктовой команде. Первая задача — проанализировать воронку верификации пользователя (KYC).
>Нужно выявить узкие места и предложить рекомендации по улучшению.

**Описать, какими шагами ты построишь анализ**

1) Сбор информации и подготовка данных
2) Построение продуктовой воронки
3) Сегментация (гео, устройства, пол, новые/старые, браузер, версия и тд)
4) Поиск слабых мест (где конверсия меньше всего)
5) Объяснение причин (UX, техпроблемы, сложность требований, доверие пользователей) - проверить через логи, ошибки, поведенческие метрики и пользовательские исследования

# [3] Кейс 3. Падение конверсии: регистрация → первый депозит

>На дашборде отмечено падение конверсии регистрация → первая оплата за последний месяц.
>Верхнеуровневая аналитика не объясняет причины.

**Расписать план расследования: какие гипотезы проверять**

- Проверка корректности данных
- Нет ли изменений в продукте/релизов/выкатки новых фич
- Не закупили ли новый трафик
- Не случился ли тех сбой в продукте

**как сегментировать**

- по устройствам 
- по ОС
- по гео
- по каналам привлечения
- новые vs возвращающиеся пользователи

**какие источники данных подключать**

- event логи
- платежные логи
- бэкэнд логи ошибок
- данные по маркетнгу
- история релизов

**как работать с платёжными системами**

- анализ отказов по причинам
- анализ отказов по сегменам
- сравнение процента успешных оплат
- проверка изменений у провайдеров
- логи

**как исключать ошибки данных и баги**

- сверка метрик из разных источников
- проверка событий на тестовом аккаунте
- сравнение поведения разных платформ

# [4] Кейс 4. Приоритизация нескольких A/B‑тестов

>Продуктовая команда принесла 3 будущих эксперимента. Все выглядят полезными. Ресурсный потолок — можно запускать только 1–2.

>Тест A — улучшение оформления платежа.
>Тест B — новая механика бонусов.
>Тест C — микроизменение в UI регистрации.

**Как проанализировать тесты перед запуском?**

- оценить на какую метрику влияет тест, MDE, объём аудитории (то есть impact)
- сравнить их срочность/полезность/стоимость/срок/риски
- сравнить независимость (одновременно лучше запустить два независимых теста)
- сравнить готовность тестов к запуску (написано ли тз, согласовано ли с разработкой)

**Что проверить: конфликты, риски, влияние друг на друга, технические зависимости, мощность, стабильность метрик?**

конфликты: тесты А и В влияют на платеж, поэтому их не стоит запускать вместе
риски: платеж и бонусы - выше риск уронить выручку
мощность и стабильность: хватит ли трафика для каждого теста, нет ли сезонности

# [5] Кейс 5. Что нужно знать перед началом исследования

> Продакт ставит задачу: «Нужно провести исследование и разобраться в проблеме». Вводных мало.

**Что ты спросишь?**

- Какая главная боль? Отсюда определим цель исследования
- На кого влияет эта проблема? Или на какие метрики?
- Есть у нас ограничения исследования? Может, в данных (насколько у проблемы может быть цифровой след?)/сроках (насколько срочная проблема?)/ресурсах/правовые ограничения
- Какой критерий успеха исследования?
- На каких пользователей/лиц влияет проблема? Есть ли конкретные сегменты?
- Есть ли зависимость с другими командами?
- Какой формат результата (отчет, дашборд, один вывод)?
</details>

<details>
  <summary>HH</summary>
ср

# [1] Задача SQL

 ```sql
with tmp as (select department_id, max(salary) as mx
from employee
group by department_id)

select emp_name, department_id
from tmp t join employee e using (department_id)
where e.salary = mx
 ```

 # [2] Задача SQL

 ```sql
select e1.emp_name, e1.department_id
from employee e1 join employee chef on e1.chef_emp_id = chef.emp_id 
where e1.salary > chef.salary 
 ```

# [3] Задача SQL

 ```sql
with tmp as (
  select contract_id, sum(sum) as sm
  from operations
  group by contract_id
)

select *,
sum/sm*100 as rate
from operations join tmp using (contract_id)
 ```

# [4] Задача Python

 ```python
n = 5

for i in range (1, n+1):
  print (i**2)
 ```

</details>

<details>
  <summary>МВидео</summary>
чт

# [1] Задача SQL

 ```sql
with events as (
  select user_pseudo_id, 
  date_trunc('day', to_timestamp(event_timestamp/1e6)) as event_day,
  event_name, event_timestamp
  from tbl
),
flags as (
  select user_pseudo_id, event_day,
  min(case when event_name = 'add_to_cart' then event_timestamp end) as min_cart,
  min(case when event_name = 'purchase' then event_timestamp end) as min_purchase
  from events 
  group by user_pseudo_id, event_day
)
select count(*) filter (where min_purchase > min_cart)*100.0/(count(*)) as rate
from flags
where min_cart is not null and event_day = current_date
 ```
</details>

<details>
  <summary>Магнит Блондинчик</summary>
пт

# [1] SQL-задача: пользователи, смотревшие видео 1 и 3, но не 2

 ```sql
select user_id from views
where 
user_id in (select distinct user_id from views where video_id = 1)
and user_id in (select distinct user_id from views where video_id = 3)
and user_id not in (select distinct user_id from views where video_id = 2)
 ```

И более оптимизированный вариант:

 ```sql
select user_id from views
group by user_id
having
sum(case when video_id = 1 then 1 else 0 end)>0 and
sum(case when video_id = 2 then 1 else 0 end)=0 and
sum(case when video_id = 3 then 1 else 0 end)>0
 ```

# [2] SQL-задача: помесячный retention

 ```sql
with cohort as (
  select user_id, 
  date_trunc('month', min(paid_at)) as cohort_month
  from transactions
  group by user_id),

  returns as (
    select distinct c.user_id, c.cohort_month
    from cohort c
    join transactions t
    on c.user_id = t.user_id
    and c.cohort_month + interval '1 month' = date_trunc('month', t.paid_at)
  )

  select 
  c.cohort_month, 
  c.cohort_month + interval '1 month' as next_month,
  count(distinct c.user_id) as cohort_users,
  count(distinct r.user_id) as returned_users,
  round(
    count(distinct r.user_id) *100.0 / count(distinct c.user_id)
  ) as retention_rate
  from cohort c left join returns r 
  on c.user_id = r.user_id 
  and c.cohort_month = r.cohort_month
  group by c.cohort_month
  order by c.cohort_month
 ```
# [3] Метрики и продуктовый кейс: сервис доставки

>Есть сервис доставки с двумя брендами (A и B). Есть маркетинг, операции (курьеры, логистика), продукт (приложение / сайт). Нужно назвать и структурировать ключевые метрики по направлениям:

>Маркетинг
>Операции
>Продукт / Поведение пользователей
>Финансы / Юнит-экономика

**Маркетинг**

1) Воронка: Показы, CR из просмотра в клик на рекламу, CR из захода в приложение/сайт в добавление в корзину, CR из корзины в покупку
2) Реклама: CAC, CTR, CPI (затраты/число установок)
3) Эффективность кампаний: ROMI, доля выручки с платного трафика, доля выручки с каждого канала привлечения

**Операции**

1) Кол-во заказов в целом и по сегментам
2) Среднее время доставки
3) SLA по доставке (доля заказов, доставленных вовремя)
4) Среднее кол-во заказов/выручки на курьера
5) Rate отмен
6) Rate обращений в поддержку
7) Затраты на логистику (курьеры+склад+упаковка)/кол-во заказов

**Продукт / Поведение пользователей**

1) Продуктовая воронка (конверсия заход-добавление в корзину-покупка)
2) Retention (D1, D7, D30, M1/M2)
3) AOV

**Финансы / Юнит-экономика**

1) GMV
2) ARPU/ARPPU
3) LTV, CAC

# [4] Python / Pandas: apply и фильтрация

**Метод DataFrame.apply — чем он отличается от «обычной функции» / циклов.**

Этот метод позволяет применить функцию к строкам или столбцам, но не является векторизированным и обычно медленнее, чем встроенные функции Pandas

**Почему в Pandas неэффективно итерироваться по строкам в явных циклах**

В Pandas неэффективно итерироваться по строкам, потому что это приводит к выполнению Python-кода вместо С-кода NumPy

**Какие есть способы фильтрации DataFrame**

- через задание условия в df[] (булев фильтр)
- через query
- isin
- notna, isna

# [5] ClickHouse: движки и обновление данных за N дней

**Чем ClickHouse отличается от транзакционных СУБД**

ClickHouse - это OLAP бд, которая позволяет быстро делать запросы и агрегации, потому что использует колоночное хранение, но медленно обновляет/удаляет данные

**Какие движки используются**

MergeTree - базовый движок для хранения сырых данных
ReplacingMergeTree — позволяет «заменять» записи при слиянии партиций по ключу.
SummingMergeTree — агрегирует числовые поля по ключу.
AggregatingMergeTree — хранит агрегатные состояния.

**Как обновлять данные за последние N дней (когда данные «Переехали» или поменялись)**

1) Хранить таблицу с партиционированием по дате
2) При изменении данных за N дней:
- либо выполнить ALTER TABLE ... DELETE where DATE BETWEEN 
- либо использовать таблицу ReplacingMergeTree и писать новые версии строк с большим version 
</details>

<details>
  <summary>СберЗдоровье2</summary>
сб

# [1] SQL

 ```sql
with tmp as (
  select country_name,
  extract(year from date) as yr,
  sum(income) as income
  from country join sales using (city)
  where extract(month from date) = 10
  and (extract(year from date) IN (2022, 2023))
  group by country_name, yr
  order by country_name, yr)

select country_name, 
(sum(case when yr = 2022 then income end) 
- sum(case when yr = 2023 then income end)) 
/sum(case when yr = 2022 then income end) * 100
from tmp
 ```

 # [2] SQL

 ```sql
with tmp as (select user_id,
date_trunc('dd', ts) as dd,
log_type,
date_trunc('dd', ts) -
row_number() over (partition by user_id, log_type order by date_trunc('dd', ts)) * interval '1 day' as grp
from tbl)

select user_id, 
log_type, 
min(dd) as start_date,
max(dd) as end_date,
count(*) as length
from tmp
where flg = 1
group by user_id, log_type, grp
having count(*)  >= 3
order by user_id, log_type, start_date
 ```

# [3] SQL

 ```sql
select
distinct store,
sum(amount*price) as total_sum,
avg(price) as avg_price,
count(distinct item) as d
from tbl
group by store
 ```

 # [3] Продуктовая часть

>Маркетолог запустил новый платный канал привлечения пользователей и обратился к вам спустя месяц с вопросом: 

**стоит ли оставлять этот канал или нет? Как будешь принимать решение**
Однозначно сказать нельзя, нужно сначала посмотреть, какой эффект от этого канала.

Я бы оценила эффективность канала через инкрементальный эффект - сравнила бы CAC с ожидаемым LTV пользователей из этого канала
А/Б мы уже провести не можем, поэтому можно пойти в квази-эксперимент: анализ временных рядов до и после запуска, сравнение с контрольными сегментами
Я бы посмотрела не только на рост пользователей и выручки, но и на качество трафика — retention, ARPU, средний чек — чтобы понять, не ухудшаются ли ключевые метрики продукта.
Также можно посмотреть, что за сегмент, который привлекает новый платный канал, не роняют ли они какие-то метрики (например, увеличивают конверсию, но роняют средний чек)


**1.1 Что такое LTV (Lifetime Value) и как его рассчитать?**
- это выручка с пользователя за весь период взаимодействия с продуктом
- рассчитывается как: aov * lifetime * средняя частота покупок


**1.2 Возможно ли, что мы привлекаем через этот канал пользователей, которые и так пришли бы к нам? Если да, то как это можно обнаружить и оценить?**
- да, каннибализация возможна
- обнаружить и оценить это можно методом временного отключения данного канала и просмотра, насколько упала метрика 
- также можно посмотреть, упала или нет органика (если она осталась на том же уровне, значит канал добавил новых людей)

</details>

<details>
  <summary>СмартГоризонт</summary>
вс

>Цель:
>Реализовать логику отправки email-рассылки клиентам из df1.

>Алгоритм:
>Для клиентов, у которых в df1 уже указан email, использовать его.
>Для клиентов без email в df1 взять email из df2 по следующему правилу:
>Найти все спаршенные email для данного клиента по полю phone.
>Предпочтительный вариант: взять email, который был спаршен впервые после даты закрытия клиента (closed).
>Если такого нет: взять последний спаршенный email до даты закрытия клиента.

Мой алгоритм (эффективность так себе из-за цикла)

```sql
import pandas as pd
for i in range (len(df1)):
  if df1.at[i, 'email'] is None:
    phone = df1.loc[i, 'phone']
    dt = df1.loc[i, 'closed']
    print(phone, dt)

    parsedDt = df2.loc[df2['phone'] == phone, ['email', 'parsedDt']].copy()

    if max(parsedDt['parsedDt']) < dt:
      mx = max(parsedDt['parsedDt'])
      email_value = parsedDt.loc[parsedDt['parsedDt'] == mx,'email'].iloc[0]
      df1.loc[i,'email'] = email_value

    else:
      email_value = parsedDt[parsedDt['parsedDt'] > dt]
      email_value = email_value.sort_values('parsedDt').iloc[0]
      df1.loc[i,'email'] = email_value['email']
```

Эффективный вариант

```python
# Разделяем клиентов с email и без email
df_with_email = df1[df1['email'].notna()]
df_no_email = df1[df1['email'].isna()]

# Сливаем df_no_email с df2 по phone
df_merged = df1.merge(df2, on = 'phone', how = 'left')

# Добавляем флаг: email после закрытия
df_merged['after_closed'] = df_merged['parsedDt'] > df_merged['closed']

# Берём первый email после закрытия, если есть, иначе последний до закрытия
def select_email(group):
    # Фильтруем только email, которые были спаршены после даты закрытия клиента
    after = group[group['after_closed'] == True].sort_values('parsedDt')
    if not after.empty:
      return after.iloc[0]['email']

    # Если email после закрытия нет, берём email до закрытия
    # Сортируем по дате в обратном порядке, чтобы взять самый последний до закрытия

    before = group[group['after_closed'] == False].sort_values(ascending = 'parsedDt')
    if not before.empty:
      return before.iloc[0]['email']
    
    return None

df_no_email['email'] = df_merged.groupby('phone').apply(select_email).reset_index(drop = True)
df_final = pd.concat([df_with_email, df_no_email], ignore_index=True)

```

</details>

<details>
  <summary>Яндекс 360</summary>
пн

# [1] SQL

**Задача 1: Найти дату, когда клиент впервые стал активным**

 ```sql
select min(msk_date)
from tbl
where is_active = 1
 ```

**Задача: Найти дату, когда клиент реактивировался (в данном случае — когда is_active снова стал 1 после того, как вернулся к 0)**

 ```sql

with tmp as(
  select msk_date, 
  case when lag(is_active) over (order by msk_date) = 0 and is_active = 1 then 1 else 0 end as flg),

tmp_2 as (
  select msk_date,
  row_number() over (order by msk_date) as rn
  from tmp
  where flg = 1
)

select msk_date
from tmp_2
where rn = 2
 ```

# [2] SQL

**1.1 На основе этой таблицы показать топ-10 покупателей в 2023 году, у которых общая сумма покупок за год превышает 1000 рублей**

 ```sql
SELECT customer_id, SUM(purchase_amount) as total
FROM tbl
WHERE EXTRACT(YEAR FROM msk_date) = 2023 
GROUP BY customer_id
HAVING SUM(purchase_amount) > 1000
ORDER BY total DESC
LIMIT 10
 ```

**Для каждого дня показать итоговую (накопленную) сумму покупок каждого покупателя с начала 2023 года.**

 ```sql
SELECT customer_id,
msk_date,
SUM(purchase_amount) OVER (PARTITION BY customer_id ORDER BY msk_date) as sm
FROM tbl
WHERE EXTRACT(YEAR FROM msk_date) >= 2023
 ```

**Найти максимальную сумму покупки в каждом году каждым пользователем**

  ```sql
SELECT customer_id, EXTRACT(YEAR FROM msk_date), 
MAX(purchase_amount) as mx
FROM tbl
GROUP BY EXTRACT(YEAR FROM msk_date), customer_id
 ```

# [3] SQL

**Для каждого человека (name) найти процент прироста значения value от 2022 года к 2021 году.**

```sql
with tmp as (select name,
sum(case when extract(year from date) = 2022 then coalesce(value, 0) end) as yr_2022,
sum(case when extract(year from date) = 2023 then coalesce(value, 0) end) as yr_2023
from tbl
group by name)

select name,
round((yr_2023 - yr_2022)/NULLIF(yr_2022, 0) *100, 2) as perc
from tmp
```

# [4] SQL

**Количество новых пользователей, зарегистрированных в каждом месяце.**

```sql
select DATE_TRUNC('month', reg_date) as mnt,
count(*) as cnt_new
from table_1
group by mnt
```
**Количество пользователей из таблицы регистраций (table_1), которые сделали хотя бы один заказ в первые 30 дней после своей регистрации**

```sql
select count(distinct user_id) as cnt
from table_1 t1 join table_2 t2 on t1.user_id = t2.user_id
and datediff('dd', t1.reg_date, t2.msk_date) <= 30
and t1.reg_date <= t2.msk_date
```

# [5] SQL задача из какого-то другого собеса

Задача: Найти начало и конец каждого непрерывного периода активности
| user_id | login_date |
|-|-|

Пример данных
```sql
CREATE TABLE user_sessions (
    user_id INT,
    login_date DATE
);

INSERT INTO user_sessions VALUES
(1, '2024-01-01'), (1, '2024-01-02'), (1, '2024-01-03'),
(1, '2024-01-05'), (1, '2024-01-06'), -- разрыв 1 день
(1, '2024-01-08'),                     -- разрыв 2 дня
(2, '2024-01-01'), (2, '2024-01-02');
```

```sql
with tmp as (select user_id, login_date,
login_date - row_number() over (partition by user_id order by login_date) * interval '1 day' as chk
from user_sessions)

select user_id,
min(login_date) as start_date,
max(login_date) as end_date
from tmp
group by user_id, chk
order by user_id, start_date, end_date
```
</details>

<details>
  <summary>01tech</summary>
вт

# [1] SQL

>Задача: Для таблицы user_orders (user_id, order_id, timestamp, order_value) получить первый заказ каждого пользователя.

 ```sql
select user_id, order_id, timestamp, order_value
from (
  select user_id, order_id, timestamp, order_value, 
  row_number() over (partition by user_id order by timestamp) as rn
  from user_orders
)t1
where rn = 1
 ```

# [2] Теоретические вопросы по A/B-тестированию

**1 - Что такое ошибки первого и второго рода (alpha и beta)?**

alpha - вероятность отвергнуть нулевую гипотезу, когда эффекта на самом деле нет (ложноположительная ошибка)
beta - вероятность не отвергнуть нулевую гипотезу, когда эффект есть
1 - beta - мощность

**2 - От чего зависит размер выборки в A/B-тесте?**

От MDE, alpha, beta, дисперсии

**3 -  Что такое MDE и зачем он нужен?**

MDE - минимальный эффект, который тест способен задетектить при заданных параметрах. Используется для расчета длительности и размера выборки

**4 -  Почему нельзя напрямую применять t-test к ratio-метрикам?**

Потому что тогда нарушается условие о независимости данных, так как числитель и знаменатель зависят от одних и тех же пользователей. Для них применяют линеаризацию, бутстрап, дельта-метод

**5 -   Что такое линеаризация метрики?**

Преобразование значений по специальной формуле в эквивалентную метрику среднего для того, чтобы можно было применять т -тест и z-тест

# [3] Python

>Задача: Посчитать ARPU для контрольной и тестовой группы по словарю вида {user_id: revenue}.

 ```python
def arpu(data):
  return sum(data.values())/len(data) if data else 0
 ```

</details>

<details>
  <summary>Employcity</summary>
ср

# [1] Python

**Есть большой CSV-файл, Pandas падает по памяти, но данные нужно анализировать в Python.**
1) Чтение файла по частям (chunksize)
2) Фильтрация столбцов - отбрасываем лишнее
3) Загрузить данные в БД и работать через SQL
4) Оптимизировать типы данных

**В Python есть список list1. Создаем list2 = list1. В чем риск и как его избежать?**

1) Оба имени теперь ссылаются на один объект
2) При изменении одного объекта - меняется второй
3) Решение: list1.copy() или list(list(1))

# [2] SQL

**В чем отличие WHERE и HAVING?**

WHERE - выполняется до агрегации (отбирает строки до select)
HAVING - выполняется уже после агрегации и фильтрует готовую таблицу

**Две таблицы: в одной 100 строк, в другой 20. Какое максимальное число строк может дать INNER JOIN?**

2000

# [3] Статистика

**В чем суть закона больших чисел?**

Что при бесконечном числе наблюдений наша оценка стремится к истинному значению

**Объяснить ЦПТ и привести пример.**

Если мы из любого распределения будем семплировать выборки и в каждой выборке считать мат ожидание, то распределение мат ожиданий будет стремиться к нормальному.

**Что такое доверительный интервал и как он используется?**

Это диапазон, в котором с вероятностью альфа находится истинное значение - используется для оценки эффекта и его значимости

# [4] А/Б

 **Есть старая и новая рекомендательная система. Как оценить, какая лучше?**

 Мы можем провести А/Б тест - разделить группы пользователей, основная метрика - конверсия в заказ, заградительная - средний чек, отмены

**От чего зависит размер выборки?**

От MDE, дисперсии, альфа, мощности (1 - бетта)

# [5] Продуктовые кейсы

**Как понять, что новая выдача товаров лучше старой?**

- провести А/Б тест и сравнить конверсии в заказ
</details>

<details>
  <summary>Ozon Fresh Final</summary>
чт

**1 - Продуктовый кейс: главный экран Ozon Fresh — виджет «Любимые товары»**

>Проблема: виджет «любимые товары/категории» «не суперклассно перформит». Прод предлагает заменить на «хиты»/«новинки» (пример: конкуренты). Доп. ограничение: исторически «раздела новинок/хитов» как отдельного виджета не было — прямого сравнения "как было" нет.

# [1] Что делать в ситуации: виджет «любимые» работает слабо, прод хочет заменить на «хиты/новинки». Как подойти к решению?
>Сформулировать цель изменения (какой бизнес/продуктовый эффект хотим получить).
>Предложить план аналитики/эксперимента с учетом того, что раньше такого раздела не было.

1) Какая цель изменения?
- Увеличить конверсию в открытие виджета/конверсию в заказ после виджета
2) План
- диагностика текущего решения - просмотр продуктовой воронки, посмотреть по разделам
- нужно конкретизировать проблему (что низкое - конверсия, вклад в GMV, негативный эффект?)



- Мы можем запустить А/Б эксперимент, чтобы проверить новые варианты,  разделить группы 50/50 и катить 2 недели, чтобы достичь MDE

# [2] Выбор метрик: интервьюер предложил «конверсия в покупку». Расшифровать числитель/знаменатель и предложить набор метрик.

Конверсия в покупку - кол-во покупок/кол-во пользователей

Можем для теста взять такие метрики
Целевая метрика: конверсия в добавление в корзину из виджета, конверсия в заказ, конверсия в открытие виджета
Заградительная: средний чек, churn, жалобы, конверсии у смежных разделов (чтобы не каннибализировать)
Информационные: среднее время на странице

# [3] Эксперимент: какие группы, как распределяем, как выбираем длительность, что такое MDE и зачем он нужен?

Группы классически 50/50 на всех пользователей, длительность берем базовую 2 недели, учитываем сезонность
MDE - минимально детектируемый эффект

# [4] Статкритерий: интервьюер уточнил, что t-test “для средних”, а конверсия — пропорция. Что использовать?

z-тест для пропорций

# [5] Контроль качества эксперимента: что проверить до запуска и во время теста (A/A, перекосы, “ничего не упало”)?

- А/А тесты, что критерий выбран корректно и сплитинг работает как нужно
- корректность событий
- после теста SRM, тех метрики, сравнение распределений по базовым признакам
</details>

<details>
  <summary>WB</summary>

# Задача 1. Актуальная цена товара на заданную дату

>Есть лог цен (история цен) с датой обновления. Нужно получить актуальное состояние товаров (цен) на первую минуту/момент времени, с учетом ограничения по дате (все что позже — неинтересно)

```sql
with tmp as (select product, 
price,
dd,
row_number() over (partition by product order by dd desc) as rn
from tbl
where dd <= 'здесь дата среза')

select product, 
price,
dd
from tmp 
where rn = 1
```

# Задача 2. Подтянуть «актуальную на дату» запись через коррелированный подзапрос

>Есть таблица ORDERS и еще одна таблица с датой обновления (интервьюер акцентировал, что это как раз кейс для коррелированного подзапроса). Нужно соединить так, чтобы для каждого заказа/объекта подтягивалась запись с максимальной update_dt, но не позже даты заказа (меньше или равно). Не забыть DISTINCT

```sql
select distinct order_id, order_date, product_id, customer_id, amount, 
  (select price
  from price_updates as pu 
  where pu.product_id = o.product_id and pu.update_dt <= o.order_date
  order by pu.update_dt desc
  limit 1) as price
from orders as o
```

# Вопрос. Способы объединения DataFrame

>Методы объединения в pandas двух датафреймов, какие знаешь?

- merge (аналог join в sql по ключам)
- join (по индексу или колонке)
- concat (склейка по строкам или колонкам)

# Задача. Палиндром

>Знаешь, что такое палиндром? Напиши функцию, которая проверяет строку и возвращает True/False. Усложнение: разный регистр (заглавные/строчные)

```python
def palindrom(s):
  s = s.lower()
  return s == s[::-1]
```

# SQL и DWH базовая теория

>Вопрос. Какие оконные функции знаешь и где применяются

1) Аггрегирующие (sum, avg, count, min, max)
2) Ранжирующие (row_number, rank, dense_rank())
3) Функции смещения (lag, lead, first_value, last_value)


>Вопрос. Принципиальная разница колоночных и строчных СУБД

Колоночные (например, кликхаус) позволяют быстро выгружать данные, выполнять запросы, но гораздо дольше выполняют загрузку/удаоение данных
Строковые (PostgreSQL/MySQL) позволяют быстро загружать/удалять данные, но запросы медленнее

# Продуктовые метрики и A/B

>Вопрос. Retention и Rolling retention. Расскажи про retention: что это, как считается. Знаешь rolling retention? Где его используют?

Retention - это метрика, которая позволяет отследить возвращаемость пользователей, чем выше ретеншн, тем пользователи дольше пользуются продуктом. Считается как 1 - churn или (кол-во юзероввернувшихся на N день / кол-во юзеров в когорте)*100

Rolling retention показывает, какой процент пользователей вернулся НА или ПОСЛЕ указанного дня. Полезен в продуктах с редкой покупкой/активацией, где "вернулся после N" релевантнее, чем "вернулся ровно в N"


>Вопрос. Как оценивать влияние новой фичи (через A/B и не только)

- A/B
- casual inference (diff in diff, временные ряды)

если мы про А/В, то что важно:
- выбор метрики, которая будет отражать эффект (основные, заградительные и информационные)
- дизайн эксперимента (считаем кол-во трафика, длительность, единица рандомизации)
- расчет показателей исходя из MDE
- проверка сплитования (А/А)
- выбор теста
- мониторинг и интерпретация

>Кейс. Какие метрики предложишь для чат-поддержки. Есть чат-поддержка (бот → оператор). Какие метрики придумал бы?

1) Пользовательские

- % открытий чата
- % успешных диалогов 
- % вопросов, которые закрыл бот
- NPS

2) Технические
- среднее время ответа бота
- среднее время ответа оператора
- среднее время с открытия чата до решения вопроса

- Динамика обращений

</details>

<details>
  <summary>Вкуссвилл</summary>

# [1] SQL
>Надо найти сумму заказов пользователей за последний месяц, совершивших заказов за последний (для пользователя) месяц больше чем за предпоследний.

```sql
with tmp as (
  select user_id,
  date_trunc('mm', order_date) as mnth,
  count(order_id) as cnt,
  sum(order_price) as sm
  from orders
  group by user_id, mnth
),
tmp_2 as (select user_id,
mnth, 
lag(cnt) over (partition by user_id order by mnth) as cnt_lag,
row_number() over (partition by user_id order by mnth desc) as rn,
cnt, sm
from tmp)

select user_id, mnth, sm
from tmp_2
where cnt > cnt_lag and rn = 1
```

# [2] Python

>Написать функцию, которая будет выводить товары, не прошедшие минимальный порог по продажам за месяц.

```python
def plan (array, plan):
  result = []
  for i in range(len(array)):
    for i in array:
      for g in i:
        if g['quantity'] < plan:
          result.append(g['product_id'])
print(list(set(result)))
```
# [3] Теорвер

>Мы продвигаем товары наших брендов в поисковой выдаче.
>У нас есть 2 опции — показывать один товар на каждые 25 выданных или заменять товар в выдаче на продвигаемый с шансом 4%.
>Какое кол-во показов мы сделаем на 100 выданных товаров. 

- 1 опция
100/25 = 4 товара на 100 выданных

- 2 опция
считаем мат ожидание
0.04*100 = 4

>При выборе второй стратегии каков шанс показать только 1 товар на 100 выданных?
Это биномиальное распределение
P(X = k) = C(n,k) × p^k × (1-p)^(n-k)
C(n,k) = n! / (k! × (n-k)!) = 100!/(1!*99!)
p^k = 0.04^1 = 0.04
(1-p)^(n-k) = 0.96^99
Вероятность показать ровно 1 наш товар из 100 ≈ 7.03% или 0.0703

# [4] Кейс

>На корзине мы имеем блок с товарами Зеленого Ценника (механика 40% скидки на товары, у которых подходит срок годности).
>Такие товары интересны пользователям. Мы планируем сделать фичу, которая будет подсвечивать эти ценники.
>Какие бы риски ты оценивал при проработке этой задачи и как бы предложил с ними бороться?

Экономические - продуктовые
- каннибализация прибыли других продуктов, которые тоже могут портиться

Технические
- неправильна работа алгоритма

Социальные
- Рост обращний в поддержку из-за практически истекшего срока годности 

И оценка по веротяности и последствиям (матрице)


</details>

<details>
  <summary>ППР</summary>

# Задача 1. Top-2 продаж для каждого клиента

> Есть таблица с полями: client_id, sale_date, amount. Нужно получить по каждому клиенту 2 самые крупные покупки

```sql
with tmp as (
  select client_id, amount, sale_date,
  row_number() over (partition by client_id order by amount desc) as rn
  from tbl
)
select client_id, amount, sale_date
from tmp
where rn <= 2
```
# Задача 2. Все покупки первого дня клиента

>Нужно вывести все строки покупок первого дня для каждого клиента

```sql
with tmp as (
  select client_id, amount, sale_date,
  dense_rank() over (partition by client_id order by sale_date) as rn
  from tbl
)
select client_id, amount, sale_date
from tmp
where rn = 1
```
# Задача 3. Полный набор месяцев для каждого клиента

>Есть агрегированные данные по месяцам. Нужно, чтобы у каждого клиента были все 12 месяцев (с 0, если продаж не было)
```sql
with tmp as (
  select generate_series(1, 12) as mnth
),
client as(
  select distinct client_id as cln from tbl 
),
crossy as (
  select cln, mnth
  from tmp cross join client
)

select cln, mnth, coalesce(sum(amount), 0) as sm
from crossy c left join tbl t on c.cln = t.client_id and c.mnth = extract(month from t.sale_date)
group by  cln, mnth
```

# Задача 4. JOIN: минимальное и максимальное число строк

> Есть таблица A (10 строк) и таблица B (5 строк). Выполняется JOIN по условию. Какое минимальное и максимальное число строк возможно?

Inner join (join) - min: 0    max: 50
Left join - min: 10    max: 50
Right join - min: 5    max: 50
Cross join - min: 50    max: 50
Full join - min: 15    max: 50

# ETL / DWH

>Слои данных (RAW, ODS, DDS)

RAW - сырые данные, которые заливаются первым слоем в двх
ODS - уже почищенные и пройденные первичную обработку данные
DDS - агрегированные данные, готовая для использования

>DWH vs Data Lake
DWH - это единое хранилище данных, организованное по слоям, данные приведены к единой модели
Data Lake - хранилище сырых данных любого типа. гибкое, но нужна обработка

>ETL vs ELT
ETL - extract, transform, load (трансформация до загрузки)
ELT - extract, load, transform (трансформация после загрузки) - если хранилище мощное

>Полная загрузка vs Incremental
Полная загрузка - сразу все данные загружаются, дорого по ресурсам
Incremental - загрузка по частям (новые, изменившиеся записи), сложнее реализовать

# Python / Визуализация

>Как визуализировать распределение сумм транзакций

- если по сегментам, то столбчатая диаграмма, можно выбрать топ-10
- если по датам, то линейная диаграмма, чтобы видеть динамику

>Histogram, Boxplot, Line chart
- гистограмма для просмотра распределения
- боксплот для просмотра статистики, медианы, квантилей, выбросов
- лайн чарт для просмотра динамики

>Работа со скошенными распределениями 
- можно сегментировать
- можно логарифмировать
- обрезать тяжелый хвост (выбросы)

</details>

<details>
  <summary>Райфайзен</summary>

# MIN / MAX число строк при JOIN (2 таблицы по 7 строк)

> MIN / MAX число строк при JOIN (2 таблицы по 7 строк)

Inner join
min - 0
max - 49

Left join
min - 7
max - 49

Right join
min - 7
max - 49

Cross join
min - 49
max - 49

Full join
min - 14
max - 49

> Допущение: join идёт по ключу. Что изменится, если в одной из таблиц у одной строки ключ равен NULL (и/или NULL есть в обеих таблицах)?
В SQL NULL не равен NULL, поэтому строки с NULL в join-ключе не будут соединены по условию.
Для LEFT/RIGHT/FULL JOIN они всё равно попадут в результат, но столбцы другой таблицы будут NULL.
Для INNER JOIN строки с NULL в ключе не попадут в результат.
Для CROSS JOIN это не влияет, потому что join идёт декартово.

# «Запрос долго работает»: почему плохо и как оптимизировать
> Есть большой запрос по банковским таблицам, он долго работает. Почему это плохо?
Потому что тратит много ресурсов, это дорого. К тому же, может положить бд, другие запросы не будут обрабатываться

>Что будешь делать, чтобы ускорить? Какие инструменты использовать?
- ограничение запроса, выполнение по частям, например, по месяцам/годам
- минимальное использование джойнов, подзапросов, условий
- использование индексом
- просмотр плана запроса через explain

# Строчные vs колоночные СУБД
> Объясни разницу между строчными (row-oriented) и колоночными (column-oriented) базами: плюсы/минусы и где что применять.

Строчные (постгрес, май скл) - читает таблицу по строкам 
минусы: долго выполняет запросы
плюсы: быстро можно загружать/удалять данные

Строчные (постгрес, май скл) - читает таблицу по столбцам 
плюсы: быстро выполняет запросы
минусы: долго загружать/удалять данные

# Найти неактивных пользователей
> Дано: таблица A — все пользователи (id), таблица B — активные пользователи (id). Получить только неактивных.

```sql
select id from A
where id not in (
  select id from B
)
```
# Доля выручки пользователя по стране

> Схема из интервью: orders(order_id, user_id, country, amount, status). Учитывать только успешные заказы. Нужно для каждого (user_id, country) посчитать долю выручки пользователя в этой стране от суммарной выручки по стране.

```sql
with tmp as( select
  country,
  sum(amount) as sm
  from orders
  where status = 'ok'
  group by country
)
select user_id, country,
  sum(amount)/sm*100 as perc
  from orders join tmp using (country)
  where status = 'ok'
  group by user_id, country
```

# Python
# Изменяемые vs неизменяемые типы
>Какие типы данных изменяемые/неизменяемые? Что это означает на уровне ссылок/памяти?

изменяемые: list, dict, set
неизменяемые: float, int, str, tuple, bytes, bool

на уровне ссылок это означает, что если несколько переменных ссылаются на один изменяемый объект, то изменение объекта будет видно через все ссылки. Для неизменяемых любое изменение приводит к созданию нового объекта

# Ссылки на один список и копирование
>Интервьюер показал кейс: a=[1,2,3]; b=a; a[0]=0. Почему меняется b и как сделать, чтобы не менялся?

теперь b=[0,2,3], потому что a и b указывают на один и тот же объект в памяти, а так как список изменяемый, то изменение отражается на обеих ссылках. Чтобы b не менялся, нужно создать не ссылку, а копию списка b = a.copy()

# Словарь: что это и почему ключи должны быть hashable
Словарь - это хеш-таблица, который хранит пару ключ-значение. Ключ должен быть хэшбл, чтобы не сломать поиск, так как необходимо иметь неизменяемый хэш

# Подсчёт частоты слов в тексте
>Нужно посчитать, сколько раз встречается каждое слово в тексте (с оговоркой про пунктуацию/регистр).

```python
import re
from collections import Counter

text.lower()
tokens = re.findall(r"[A-Za-zА-Яа-я0-9_]+", text.lower())
freq = Counter(tokens)
```
# Проверка: все ли символы в строке уникальны
```python
len(s) == len(set(s))
```

# Вернуть два максимума из массива (с учётом повторов)
> Интервьюер просил функцию: вернуть два самых больших числа; если максимум повторяется — вернуть его дважды.
```python
def func(l):
  l = sorted(l, reverse = True)
  return l[:2]
```
# Pandas
# Как вывести количество строк в DataFrame

```python
df.info()
```                                         
```python
len(df)
``` 
# Как прочитать CSV в Pandas
```python
import pandas as pd
df = pd.read_csv('---')
``` 
# Группировка: сумма сделок по клиенту
# Дано: client_id (уникальный идентификатор клиента) и deal_amount (сумма сделки). Посчитать сумму по каждому клиенту.

```python
df = df.groupby('client_id')['deal_amount'].sum()
``` 
# Как groupby ведёт себя с NaN
Nan исключается, можно включить df = df.groupby('client_id', dropna = False)['deal_amount'].sum()

# Доля количества сделок по клиенту от общего числа сделок

```python
cnt = df.groupby('client_id')['deal_amount'].count()
result = (cnt/len(df)).reset_index()
``` 
</details>

<details>
  <summary>Uzum</summary>


</details>


<details>
  <summary>Uzum Кейс</summary>


</details>

<details>
  <summary>Yandex</summary>


</details>

<details>
  <summary>x5 техничка</summary>


</details>

<details>
  <summary>Вкуссвилл финал</summary>


</details>

<details>
  <summary>Доксинбоксин</summary>


</details>

<details>
  <summary>Райфайзен кейсы</summary>


</details>

</details>